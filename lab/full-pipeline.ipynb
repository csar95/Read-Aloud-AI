{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bc548e8",
   "metadata": {},
   "source": [
    "# Full pipeline implementation\n",
    "\n",
    "**Description**: This notebook implements a full pipeline for extracting text from a PDF file, formatting it, and then using it to generate audio using a text-to-speech (TTS) model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e824b00",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02d16645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from IPython.display import display, Audio\n",
    "from kokoro import KModel, KPipeline\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import pypdfium2 as pdfium\n",
    "import soundfile as sf\n",
    "\n",
    "from src.io_schemas.output_schemas import FormattedPageText\n",
    "from src.io_schemas.prompts import FORMAT_TEXT_FOR_TTS\n",
    "from src.pdf_reader.helpers import detect_header_footer\n",
    "from src.openai_api_utils.controller import OpenAIAPIController\n",
    "from src.utils.custom_exceptions import OpenAIInvalidResponseFormatError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bd4e341",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"./data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08677124",
   "metadata": {},
   "source": [
    "## Setup Gemini client and OpenAI API controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95520b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "model_id = \"gemini-2.5-flash-preview-04-17\"  # \"gemini-2.5-pro-exp-03-25\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a69283b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [{'id': 'models/chat-bison-001',\n",
       "   'object': 'model',\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'models/text-bison-001', 'object': 'model', 'owned_by': 'google'},\n",
       "  {'id': 'models/embedding-gecko-001',\n",
       "   'object': 'model',\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-1.0-pro-vision-latest',\n",
       "   'object': 'model',\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-pro-vision', 'object': 'model', 'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-1.5-pro-latest',\n",
       "   'object': 'model',\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-1.5-pro-001', 'object': 'model', 'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-1.5-pro-002', 'object': 'model', 'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-1.5-pro', 'object': 'model', 'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-1.5-flash-latest',\n",
       "   'object': 'model',\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-1.5-flash-001',\n",
       "   'object': 'model',\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-1.5-flash-001-tuning',\n",
       "   'object': 'model',\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-1.5-flash', 'object': 'model', 'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-1.5-flash-002',\n",
       "   'object': 'model',\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-1.5-flash-8b',\n",
       "   'object': 'model',\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-1.5-flash-8b-001',\n",
       "   'object': 'model',\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-1.5-flash-8b-latest',\n",
       "   'object': 'model',\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-1.5-flash-8b-exp-0827',\n",
       "   'object': 'model',\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-1.5-flash-8b-exp-0924',\n",
       "   'object': 'model',\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-2.5-pro-exp-03-25',\n",
       "   'object': 'model',\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-2.5-pro-preview-03-25',\n",
       "   'object': 'model',\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-2.5-flash-preview-04-17',\n",
       "   'object': 'model',\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-2.5-flash-preview-04-17-thinking',\n",
       "   'object': 'model',\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-2.5-pro-preview-05-06',\n",
       "   'object': 'model',\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-2.0-flash-exp',\n",
       "   'object': 'model',\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-2.0-flash', 'object': 'model', 'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-2.0-flash-001',\n",
       "   'object': 'model',\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-2.0-flash-lite-001',\n",
       "   'object': 'model',\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-2.0-flash-lite',\n",
       "   'object': 'model',\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-2.0-flash-lite-preview-02-05',\n",
       "   'object': 'model',\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-2.0-flash-lite-preview',\n",
       "   'object': 'model',\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-2.0-pro-exp', 'object': 'model', 'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-2.0-pro-exp-02-05',\n",
       "   'object': 'model',\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-exp-1206', 'object': 'model', 'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-2.0-flash-thinking-exp-01-21',\n",
       "   'object': 'model',\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-2.0-flash-thinking-exp',\n",
       "   'object': 'model',\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-2.0-flash-thinking-exp-1219',\n",
       "   'object': 'model',\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'models/learnlm-2.0-flash-experimental',\n",
       "   'object': 'model',\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'models/gemma-3-1b-it', 'object': 'model', 'owned_by': 'google'},\n",
       "  {'id': 'models/gemma-3-4b-it', 'object': 'model', 'owned_by': 'google'},\n",
       "  {'id': 'models/gemma-3-12b-it', 'object': 'model', 'owned_by': 'google'},\n",
       "  {'id': 'models/gemma-3-27b-it', 'object': 'model', 'owned_by': 'google'},\n",
       "  {'id': 'models/embedding-001', 'object': 'model', 'owned_by': 'google'},\n",
       "  {'id': 'models/text-embedding-004', 'object': 'model', 'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-embedding-exp-03-07',\n",
       "   'object': 'model',\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-embedding-exp',\n",
       "   'object': 'model',\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'models/aqa', 'object': 'model', 'owned_by': 'google'},\n",
       "  {'id': 'models/imagen-3.0-generate-002',\n",
       "   'object': 'model',\n",
       "   'owned_by': 'google'},\n",
       "  {'id': 'models/gemini-2.0-flash-live-001',\n",
       "   'object': 'model',\n",
       "   'owned_by': 'google'}],\n",
       " 'object': 'list'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = OpenAI(\n",
    "    api_key=GEMINI_API_KEY,\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
    ")\n",
    "client.models.list().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a120c59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_controller = OpenAIAPIController(\n",
    "    openai_client=client,\n",
    "    model_name=model_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31953225",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_kwargs = {\n",
    "    # \"max_completion_tokens\": 30_000,\n",
    "    \"temperature\": 0.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130cd08d",
   "metadata": {},
   "source": [
    "## Download and initialize TTS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f9bd7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_ID = \"hexgrad/Kokoro-82M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e057aaac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/kokoro/kokoro-v1_0.pth'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tts_model_path = hf_hub_download(\n",
    "    repo_id=REPO_ID,\n",
    "    filename=KModel.MODEL_NAMES[REPO_ID],\n",
    "    local_dir=\"./models/kokoro\",\n",
    "    force_download=False,  # Set to True to force redownload even if the file exists\n",
    ")\n",
    "tts_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "210f57a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/csr95/Desktop/Mis_Documentos/Code_Python/Read-Aloud-AI/.venv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/Users/csr95/Desktop/Mis_Documentos/Code_Python/Read-Aloud-AI/.venv/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "tts_model = KModel(repo_id=REPO_ID, model=tts_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e1af2a",
   "metadata": {},
   "source": [
    "## Load PDF document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "359d28de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of PDF: 34 pages\n"
     ]
    }
   ],
   "source": [
    "pdf_path = DATA_DIR / \"pdf_docs/a-practical-guide-to-building-agents.pdf\"\n",
    "pdf = pdfium.PdfDocument(pdf_path)\n",
    "print(f\"Length of PDF: {len(pdf)} pages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d392ae7",
   "metadata": {},
   "source": [
    "## Extract text from document pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91d42c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['33 A practical guide to building agents',\n",
       " '4 A practical guide to building agents',\n",
       " '53']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header_footer_lines = detect_header_footer(document=pdf)\n",
    "list(header_footer_lines)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "060a4921",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_from_pages = []\n",
    "\n",
    "for page_id in range(len(pdf)):\n",
    "    # It seems that the package \"pypdfium2\" separates lines by \"\\r\\n\" by default\n",
    "    page_text = pdf[page_id].get_textpage().get_text_bounded()\n",
    "\n",
    "    # Remove lines contained in header/footer\n",
    "    page_text_without_header_footer = \"\\n\".join(\n",
    "        line\n",
    "        for line in page_text.splitlines()\n",
    "        if line.strip() not in header_footer_lines\n",
    "    )\n",
    "\n",
    "    text_from_pages.append(page_text_without_header_footer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdb61cc",
   "metadata": {},
   "source": [
    "## Use the LLM to format the extracted text into a text suitable for TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b818c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SILENCE_KEYWORD = \"[SILENCE]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6dda7176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------\n",
      "Processing page 1/34\n",
      "Received response from OpenAI API. Response: ParsedChatCompletion[FormattedPageText](id=None, choices=[ParsedChoice[FormattedPageText](finish_reason='stop', index=0, logprobs=None, message=ParsedChatCompletionMessage[FormattedPageText](content='{\\n  \"text\": \"[SILENCE] A practical guide to building agents.\"\\n}', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, parsed=FormattedPageText(text='[SILENCE] A practical guide to building agents.')))], created=1747481717, model='gemini-2.5-flash-preview-04-17', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=19, prompt_tokens=813, total_tokens=1063, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "num_attempts: 1\n",
      "response_time (s): 2.29\n",
      "-------------------------------------------------------------------\n",
      "Processing page 2/34\n",
      "Received response from OpenAI API. Response: ParsedChatCompletion[FormattedPageText](id=None, choices=[ParsedChoice[FormattedPageText](finish_reason='stop', index=0, logprobs=None, message=ParsedChatCompletionMessage[FormattedPageText](content='{\\n  \"text\": \"[SILENCE] This guide will cover several key areas. We\\'ll start by defining what an agent is, then discuss when you should consider building one. We\\'ll delve into the foundations of agent design, explore guardrails, and finally, offer a conclusion.\"\\n}', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, parsed=FormattedPageText(text=\"[SILENCE] This guide will cover several key areas. We'll start by defining what an agent is, then discuss when you should consider building one. We'll delve into the foundations of agent design, explore guardrails, and finally, offer a conclusion.\")))], created=1747481720, model='gemini-2.5-flash-preview-04-17', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=62, prompt_tokens=965, total_tokens=1449, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "num_attempts: 1\n",
      "response_time (s): 3.39\n",
      "-------------------------------------------------------------------\n",
      "Processing page 3/34\n",
      "Received response from OpenAI API. Response: ParsedChatCompletion[FormattedPageText](id=None, choices=[ParsedChoice[FormattedPageText](finish_reason='stop', index=0, logprobs=None, message=ParsedChatCompletionMessage[FormattedPageText](content='{\\n  \"text\": \"[SILENCE] Let\\'s begin with the introduction. Large language models are becoming increasingly capable of handling complex, multi-step tasks. Advances in reasoning, multimodality, and tool use have unlocked a new category of LLM-powered systems known as agents. This guide is designed for product and engineering teams exploring how to build their first agents, distilling insights from numerous customer deployments into practical and actionable best practices. It includes frameworks for identifying promising use cases, clear patterns for designing agent logic and orchestration, and best practices to ensure your agents run safely, predictably, and effectively. After reading this guide, you\\'ll have the foundational knowledge you need to confidently start building your first agent. [SILENCE]\"\\n}', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, parsed=FormattedPageText(text=\"[SILENCE] Let's begin with the introduction. Large language models are becoming increasingly capable of handling complex, multi-step tasks. Advances in reasoning, multimodality, and tool use have unlocked a new category of LLM-powered systems known as agents. This guide is designed for product and engineering teams exploring how to build their first agents, distilling insights from numerous customer deployments into practical and actionable best practices. It includes frameworks for identifying promising use cases, clear patterns for designing agent logic and orchestration, and best practices to ensure your agents run safely, predictably, and effectively. After reading this guide, you'll have the foundational knowledge you need to confidently start building your first agent. [SILENCE]\")))], created=1747481724, model='gemini-2.5-flash-preview-04-17', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=151, prompt_tokens=1203, total_tokens=1633, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "num_attempts: 1\n",
      "response_time (s): 3.34\n",
      "-------------------------------------------------------------------\n",
      "Processing page 4/34\n",
      "Received response from OpenAI API. Response: ParsedChatCompletion[FormattedPageText](id=None, choices=[ParsedChoice[FormattedPageText](finish_reason='stop', index=0, logprobs=None, message=ParsedChatCompletionMessage[FormattedPageText](content='{\\n  \"text\": \"What exactly is an agent? [SILENCE] While conventional software helps users streamline and automate workflows, agents are capable of performing these same workflows on the user\\'s behalf with a high degree of independence. Essentially, agents are systems that independently accomplish tasks for you. A workflow is simply a sequence of steps that must be executed to meet a user\\'s goal, whether that\\'s resolving a customer service issue, booking a restaurant reservation, committing a code change, or generating a report. It\\'s important to note that applications integrating large language models but not using them to control workflow execution—like simple chatbots, single-turn LLMs, or sentiment classifiers—are not considered agents. [SILENCE] More concretely, an agent possesses core characteristics that allow it to act reliably and consistently on behalf of a user. First, it leverages a large language model to manage workflow execution and make decisions. It can recognize when a workflow is complete and proactively correct its actions if needed. In case of failure, it can halt execution and transfer control back to the user. Second, it has access to various tools to interact with external systems, both to gather context and to take actions. It dynamically selects the appropriate tools depending on the workflow\\'s current state, always operating within clearly defined guardrails.\"\\n}', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, parsed=FormattedPageText(text=\"What exactly is an agent? [SILENCE] While conventional software helps users streamline and automate workflows, agents are capable of performing these same workflows on the user's behalf with a high degree of independence. Essentially, agents are systems that independently accomplish tasks for you. A workflow is simply a sequence of steps that must be executed to meet a user's goal, whether that's resolving a customer service issue, booking a restaurant reservation, committing a code change, or generating a report. It's important to note that applications integrating large language models but not using them to control workflow execution—like simple chatbots, single-turn LLMs, or sentiment classifiers—are not considered agents. [SILENCE] More concretely, an agent possesses core characteristics that allow it to act reliably and consistently on behalf of a user. First, it leverages a large language model to manage workflow execution and make decisions. It can recognize when a workflow is complete and proactively correct its actions if needed. In case of failure, it can halt execution and transfer control back to the user. Second, it has access to various tools to interact with external systems, both to gather context and to take actions. It dynamically selects the appropriate tools depending on the workflow's current state, always operating within clearly defined guardrails.\")))], created=1747481727, model='gemini-2.5-flash-preview-04-17', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=269, prompt_tokens=1190, total_tokens=1676, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "num_attempts: 1\n",
      "response_time (s): 3.80\n"
     ]
    }
   ],
   "source": [
    "formatted_document_text = \"\"\n",
    "for page_id, page_text in enumerate(text_from_pages[:4]):\n",
    "    print((\"-------------------------------------------------------------------\"))\n",
    "    print(f\"Processing page {page_id + 1}/{len(text_from_pages)}\")\n",
    "\n",
    "    # Get input texts that are needed to build the prompt\n",
    "    previous_fragment = (\n",
    "        f\"... {formatted_document_text[-100:]}\" if formatted_document_text else \"\"\n",
    "    )\n",
    "    current_page = page_text\n",
    "    next_preview = (\n",
    "        text_from_pages[page_id + 1] if page_id + 1 < len(text_from_pages) else \"\"\n",
    "    )\n",
    "\n",
    "    # Build the prompt object as the OpenAI API controller expects\n",
    "    prompt = {\n",
    "        \"system_msg\": FORMAT_TEXT_FOR_TTS.system_msg.format(\n",
    "            silence_keyword=SILENCE_KEYWORD,\n",
    "        ),\n",
    "        \"user_msg\": FORMAT_TEXT_FOR_TTS.user_msg.format(\n",
    "            previous_fragment=previous_fragment,\n",
    "            current_page=current_page,\n",
    "            next_preview=next_preview,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    # Send the request to the OpenAI/Gemini API\n",
    "    chat_completion, elapsed_time_s, retries_taken = openai_api_controller.send_request(\n",
    "        prompt=prompt,\n",
    "        response_format=FORMAT_TEXT_FOR_TTS.output_json,\n",
    "        **openai_api_kwargs,\n",
    "    )\n",
    "    num_attempts = retries_taken + 1\n",
    "    print(\n",
    "        f\"Received response from OpenAI API. Response: {chat_completion}\\n\"\n",
    "        f\"num_attempts: {num_attempts}\\n\"\n",
    "        f\"response_time (s): {elapsed_time_s:.2f}\"\n",
    "    )\n",
    "\n",
    "    # Validate response format. Raises custom exception if the response does not match the `FormattedPageText` schema\n",
    "    response_msg = chat_completion.choices[0].message.content\n",
    "    try:\n",
    "        formatted_page_text = FormattedPageText(**json.loads(response_msg))\n",
    "    except Exception as e:\n",
    "        print(f\"Error validating response format: {str(e)}\")\n",
    "        raise OpenAIInvalidResponseFormatError()\n",
    "    \n",
    "    formatted_document_text += f\" {formatted_page_text.text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26ed5d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A practical guide to building agents.',\n",
       " \"This guide will cover several key areas. We'll start by defining what an agent is, then discuss when you should consider building one. We'll delve into the foundations of agent design, explore guardrails, and finally, offer a conclusion.\",\n",
       " \"Let's begin with the introduction. Large language models are becoming increasingly capable of handling complex, multi-step tasks. Advances in reasoning, multimodality, and tool use have unlocked a new category of LLM-powered systems known as agents. This guide is designed for product and engineering teams exploring how to build their first agents, distilling insights from numerous customer deployments into practical and actionable best practices. It includes frameworks for identifying promising use cases, clear patterns for designing agent logic and orchestration, and best practices to ensure your agents run safely, predictably, and effectively. After reading this guide, you'll have the foundational knowledge you need to confidently start building your first agent.\",\n",
       " 'What exactly is an agent?',\n",
       " \"While conventional software helps users streamline and automate workflows, agents are capable of performing these same workflows on the user's behalf with a high degree of independence. Essentially, agents are systems that independently accomplish tasks for you. A workflow is simply a sequence of steps that must be executed to meet a user's goal, whether that's resolving a customer service issue, booking a restaurant reservation, committing a code change, or generating a report. It's important to note that applications integrating large language models but not using them to control workflow execution—like simple chatbots, single-turn LLMs, or sentiment classifiers—are not considered agents.\",\n",
       " \"More concretely, an agent possesses core characteristics that allow it to act reliably and consistently on behalf of a user. First, it leverages a large language model to manage workflow execution and make decisions. It can recognize when a workflow is complete and proactively correct its actions if needed. In case of failure, it can halt execution and transfer control back to the user. Second, it has access to various tools to interact with external systems, both to gather context and to take actions. It dynamically selects the appropriate tools depending on the workflow's current state, always operating within clearly defined guardrails.\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks = [\n",
    "    chunk.strip()\n",
    "    for chunk in formatted_document_text.split(SILENCE_KEYWORD)\n",
    "    if chunk.strip() != \"\"\n",
    "]\n",
    "text_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e64c5e",
   "metadata": {},
   "source": [
    "## Pass the formatted text to the TTS model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03e38be",
   "metadata": {},
   "source": [
    "### Initialize pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07a945f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "pipeline = KPipeline(lang_code=\"a\", repo_id=REPO_ID, model=tts_model, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02076f06",
   "metadata": {},
   "source": [
    "### Generate audio from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1534f3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much silence to insert between paragraphs: 5000 is about 0.2 seconds\n",
    "SILENCE_DURATION = 0.3  # seconds\n",
    "SAMPLE_RATE = 24_000\n",
    "VOICE_ID = \"am_liam\"  # \"am_puck\" is another cool male voice.\n",
    "SPEED = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29965928",
   "metadata": {},
   "source": [
    "**Note:** This function is necessary to prevent the TTS model from speeding up the voice too much. The pipeline processes the text in chunks to generate audio of approximately 25 seconds. If any fragment is slightly longer, the TTS model may slightly accelerate the voice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a38baff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(text: str, max_words: int = 50) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits the text into chunks formed by sentences, ensuring that each chunk does not\n",
    "    exceed the specified number of words.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The text to be split into chunks.\n",
    "    max_words : int\n",
    "        The maximum number of words allowed in each chunk.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        A list of text chunks, each containing a maximum of `max_words` words.\n",
    "    \"\"\"\n",
    "    sentences_in_text = text.split(\". \")\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for sentence in sentences_in_text:\n",
    "        if len(current_chunk.split()) + len(sentence.split()) <= max_words:\n",
    "            current_chunk += sentence + (\". \" if sentence[-1] != \".\" else \"\")\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + (\". \" if sentence[-1] != \".\" else \"\")\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "        \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f8fab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_chunks = []\n",
    "\n",
    "for text_chunk_id, text_chunk in enumerate(text_chunks):\n",
    "    print(\n",
    "        \"-------------------------------\\n\"\n",
    "        f\"Processing text chunk {text_chunk_id + 1}/{len(text_chunks)}\\n\"\n",
    "        f\" Number of sentences: {text_chunk.count('.')} |\"\n",
    "        f\" Number of words: {len(text_chunk.split())}\"\n",
    "    )\n",
    "\n",
    "    smaller_text_chunks = split_into_chunks(text=text_chunk, max_words=50)\n",
    "    \n",
    "    audio_chunks_for_text_chunk = []\n",
    "    for small_text_chunk in smaller_text_chunks:\n",
    "        generator = pipeline(text=small_text_chunk, voice=VOICE_ID, speed=SPEED)\n",
    "        \n",
    "        for audio_chunk_id, (graphemes, phonemes, audio_chunk) in enumerate(generator):\n",
    "            print(\n",
    "                f\"++++ Processing audio chunk {audio_chunk_id + 1}\\n\"\n",
    "                f\" Number of words: {len(graphemes.split())}\\n\"\n",
    "                f\" Graphemes: {graphemes}\\n\"\n",
    "                f\" Phonemes: {phonemes}\"\n",
    "            )\n",
    "            display(Audio(data=audio_chunk, rate=SAMPLE_RATE))\n",
    "            audio_chunks_for_text_chunk.append(audio_chunk)\n",
    "    \n",
    "    if text_chunk_id > 0:\n",
    "        # Add silence between chunks\n",
    "        silence = np.zeros(int(SILENCE_DURATION * SAMPLE_RATE), dtype=np.float32)\n",
    "        audio_chunks_for_text_chunk = np.concatenate(\n",
    "            [silence, np.concatenate(audio_chunks_for_text_chunk)]\n",
    "        )\n",
    "    else:\n",
    "        audio_chunks_for_text_chunk = np.concatenate(audio_chunks_for_text_chunk)\n",
    "\n",
    "    audio_chunks.append(audio_chunks_for_text_chunk)\n",
    "\n",
    "final_audio = np.concatenate(audio_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a85ed01",
   "metadata": {},
   "source": [
    "### Save final audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a444a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the output directory exists\n",
    "output_dir = DATA_DIR /  \"output_audio/\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the audio to a file\n",
    "sf.write(\n",
    "    file=output_dir / \"HEARME_en.wav\", data=final_audio, samplerate=SAMPLE_RATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8666d356",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Audio(data=final_audio, rate=SAMPLE_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adefd1a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
